{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb1JI3eiMtG1IF8klqDVf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaomarcosgv/deep_learning_python/blob/main/Machine_Learning_e_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTAmChUgUnBK"
      },
      "outputs": [],
      "source": [
        "pip install plotly --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "bGELmis4VkJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##PRÉ-PROCESSAMENTO DA BASE CREDIT_DATA\n",
        "base_credit = pd.read_csv('/content/credit_data.csv')\n",
        "np.unique(base_credit['default'], return_counts=True)\n",
        "#sns.countplot(x = base_credit['default']);\n",
        "#plt.hist(x = base_credit['age']);\n",
        "#grafico = px.scatter_matrix(base_credit, dimensions=['age','income','loan'], color='default');\n",
        "#grafico.show()\n",
        "base_credit.loc[base_credit['age']<0]\n",
        "base_credit2 = base_credit.drop('age', axis = 1)\n",
        "base_credit2\n",
        "base_credit[base_credit['age'] < 0]\n",
        "base_credit3 = base_credit.drop(base_credit[base_credit['age'] < 0].index)\n",
        "base_credit3\n",
        "base_credit.mean()\n",
        "base_credit3['age'][base_credit['age'] > 0].mean()\n",
        "base_credit = pd.read_csv('/content/credit_data.csv')\n",
        "base_credit.loc[base_credit['age'] < 0, 'age'] = 40.92\n",
        "base_credit.isnull().sum()\n",
        "base_credit.loc[pd.isnull(base_credit['age'])]\n",
        "base_credit['age'].fillna(base_credit['age'].mean(), inplace = True)\n",
        "base_credit.loc[pd.isnull(base_credit['age'])]\n",
        "base_credit.loc[base_credit['clientid'].isin([29,31,32])]\n",
        "x_credit = base_credit.iloc[:, 1:4].values\n",
        "x_credit\n",
        "y_credit = base_credit.iloc[:, 4].values\n",
        "y_credit\n",
        "x_credit[:,0].min(), x_credit[:,0].max(), x_credit[:,1].min(), x_credit[:,1].max(), x_credit[:,2].min(), x_credit[:,2].max()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_credit = StandardScaler()\n",
        "x_credit = scaler_credit.fit_transform(x_credit)\n",
        "import pickle \n",
        "with open('credit.pkl', mode='wb') as f:\n",
        "  pickle.dump([x_credit_treinamento,y_credit_treinamento,x_credit_teste,y_credit_teste], f)"
      ],
      "metadata": {
        "id": "6IyfMjArYaJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##PRÉ-PROCESSAMENTO DA BASE CENSUS\n",
        "base_census = pd.read_csv('/content/census.csv')\n",
        "#base_census.describe()\n",
        "#base_census.isnull().sum()\n",
        "#grafico = px.treemap(base_census, path=['workclass', 'age'])\n",
        "#grafico.show()\n",
        "#grafico = px.parallel_categories(base_census,dimensions=['occupation','relationship','income'])\n",
        "#grafico.show()\n",
        "x_census=base_census.iloc[:,0:14].values\n",
        "y_census=base_census.iloc[:,14].values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder_teste = LabelEncoder()\n",
        "teste = label_encoder_teste.fit_transform(x_census[:,1])\n",
        "label_encoder_workclass = LabelEncoder()\n",
        "label_encoder_education = LabelEncoder()\n",
        "label_encoder_marital = LabelEncoder()\n",
        "label_encoder_occupation = LabelEncoder()\n",
        "label_encoder_relationship = LabelEncoder()\n",
        "label_encoder_race = LabelEncoder()\n",
        "label_encoder_sex = LabelEncoder()\n",
        "label_encoder_country = LabelEncoder()\n",
        "x_census[:,1] = label_encoder_workclass.fit_transform(x_census[:,1])\n",
        "x_census[:,3] = label_encoder_education.fit_transform(x_census[:,3])\n",
        "x_census[:,5] = label_encoder_marital.fit_transform(x_census[:,5])\n",
        "x_census[:,6] = label_encoder_occupation.fit_transform(x_census[:,6])\n",
        "x_census[:,7] = label_encoder_relationship.fit_transform(x_census[:,7])\n",
        "x_census[:,8] = label_encoder_race.fit_transform(x_census[:,8])\n",
        "x_census[:,9] = label_encoder_sex.fit_transform(x_census[:,9])\n",
        "x_census[:,13] = label_encoder_country.fit_transform(x_census[:,13])\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "x_census = base_census.iloc[:,0:14].values \n",
        "##Tive que chamar x_census novamente aqui, para que o toarray funcione\n",
        "onehotencoder_census = ColumnTransformer(transformers=[('OneHot', OneHotEncoder(), [1,3,5,6,7,8,9,13])], remainder='passthrough')\n",
        "x_census = onehotencoder_census.fit_transform(x_census).toarray() #x_census[:,1]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_credit = StandardScaler()\n",
        "x_credit = scaler_credit.fit_transform(x_credit)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_credit_treinamento, x_credit_teste, y_credit_treinamento, y_credit_teste = train_test_split(x_credit,y_credit,test_size=0.25,random_state=0)\n",
        "x_census_treinamento, x_census_teste, y_census_treinamento, y_census_teste = train_test_split(x_census,y_census,test_size=0.15,random_state=0)\n",
        "import pickle \n",
        "with open('census.pkl', mode='wb') as f:\n",
        "  pickle.dump([x_census_treinamento,y_census_treinamento,x_census_teste,y_census_teste], f)"
      ],
      "metadata": {
        "id": "roj3VTSj4hU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APLICAÇÃO DO ALGORITMO PARA A BASE RISCO_CREDITO\n",
        "base_risco_credito = pd.read_csv('/content/risco_credito.csv')\n",
        "x_risco_credito = base_risco_credito.iloc[:,0:4].values\n",
        "y_risco_credito = base_risco_credito.iloc[:,4].values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder_historia = LabelEncoder()\n",
        "label_encoder_divida = LabelEncoder()\n",
        "label_encoder_garantia = LabelEncoder()\n",
        "label_encoder_renda = LabelEncoder()\n",
        "x_risco_credito[:,0] = label_encoder_historia.fit_transform(x_risco_credito[:,0])\n",
        "x_risco_credito[:,1] = label_encoder_historia.fit_transform(x_risco_credito[:,1])\n",
        "x_risco_credito[:,2] = label_encoder_historia.fit_transform(x_risco_credito[:,2])\n",
        "x_risco_credito[:,3] = label_encoder_historia.fit_transform(x_risco_credito[:,3])\n",
        "import pickle\n",
        "with open('risco_credito.pkl','wb') as f:\n",
        "  pickle.dump([x_risco_credito, y_risco_credito], f)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "naive_risco_credito = GaussianNB()\n",
        "##Gerar tabela de probabilidades\n",
        "naive_risco_credito.fit(x_risco_credito, y_risco_credito)\n",
        "##historia boa, divida alta, garantias nenhuma, renda>35\n",
        "##historia ruim, divida alta, garantias adequada, renda<15\n",
        "previsao = naive_risco_credito.predict([[0,0,1,2],[2,0,0,0]])\n",
        "previsao"
      ],
      "metadata": {
        "id": "Cl_hUGXKrcWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APLICAÇÃO DO ALGORITMO PARA A BASE CREDIT_DATA\n",
        "with open('credit.pkl','rb') as f:\n",
        "  x_credit_treinamento, x_credit_teste, y_credit_treinamento, y_credit_teste = pickle.load(f)\n",
        "naive_credit_data = GaussianNB()\n",
        "naive_credit_data.fit(x_credit_treinamento, y_credit_treinamento)\n",
        "previsoes = naive_credit_data.predict(x_credit_teste)"
      ],
      "metadata": {
        "id": "ERtMtWaJ6dR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)\n",
        "confusion_matrix(y_credit_teste, previsoes)\n",
        "##from yellowbrick.classifier import ConfusionMatrix\n",
        "print(classification_report(y_credit_teste, previsoes))\n",
        "##O algoritmo consegue identificar corretamente somente 64% dos registros que não pagam\n",
        "##E quando ele identifica um cliente que não paga, ele está correto em 84% dos casos"
      ],
      "metadata": {
        "id": "_cEA4cIQCDV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##APLICAÇÃO DE REDES NEURAIS PARA A BASE CREDIT_DATA\n",
        "from sklearn.neural_network import MLPClassifier #Multilayer perceptron\n",
        "import pickle\n",
        "with open('credit.pkl','rb') as f:\n",
        "  x_credit_treinamento, x_credit_teste, y_credit_treinamento, y_credit_teste = pickle.load(f)\n",
        "x_credit_treinamento\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000001,solver='adam', activation='relu', hidden_layer_sizes=(5,5)) \n",
        "rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "id": "O1ZfBJsAE-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "previsoes = rede_neural_credit.predict(x_credit_teste)\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_I5hXe9LLej",
        "outputId": "077d2a87-4580-43d1-dafa-ad52fe4265c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkDu2U-aRPt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}